{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(SinusoidalPosEmb, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "        return emb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emb = SinusoidalPosEmb(16)\n",
    "x = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, device, t_dim = 16):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.t_dim = t_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.device = device\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(t_dim),\n",
    "            nn.Linear(t_dim, t_dim * 2),\n",
    "            nn.MIsh(),\n",
    "            nn.Linear(t_dim * 2, t_dim)\n",
    "        )\n",
    "\n",
    "        input_dim = state_dim + t_dim + action_dim\n",
    "        self.mid_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Mish(),\n",
    "        )\n",
    "        self.final_layer = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x, time, state):\n",
    "        t_emb = self.time_mlp(time)\n",
    "        x = torch.cat([x, state, t_emb], dim=1)\n",
    "        x = self.mid_layer(x)\n",
    "        return self.final_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WeightedLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, targ, weighted = 1.0):\n",
    "        loss = self._loss(pred, targ)\n",
    "        WeightedLoss = (loss * weighted).mean()\n",
    "        return WeightedLoss\n",
    "    \n",
    "class WeightedL1(WeightedLoss):\n",
    "    def _loss(self, pred, targ):\n",
    "        return torch.abs(pred - targ)\n",
    "    \n",
    "class WeightedL2(WeightedLoss):\n",
    "    def _loss(self, pred, targ):\n",
    "        return F.mse_loss(pred, targ)\n",
    "    \n",
    "Losses = {\n",
    "    'l1' : WeightedL1,\n",
    "    'l2' : WeightedL2,\n",
    "}\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1, ) * (len(x_shape) - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion(nn.Module):\n",
    "    def __init__(self, loss_type, beta_schedule = \"linear\",\n",
    "                 clip_denoised = True, predict_epsilon = True, **kwargs):\n",
    "        super(Diffusion, self).__init__()\n",
    "        self.state_dim = kwargs[\"obs_dim\"]\n",
    "        self.action_dim = kwargs[\"act_dim\"]\n",
    "        self.hidden_dim = kwargs[\"hidden_dim\"]\n",
    "        self.device = torch.device(kwargs[\"device\"])\n",
    "        self.T = kwargs[\"T\"]\n",
    "        self.clip_denoised = clip_denoised\n",
    "        self.predict_epsilon = predict_epsilon\n",
    "        self.model = MLP(self.state_dim, self.action_dim, self.hidden_dim, self.device)\n",
    "\n",
    "        if beta_schedule == \"linear\":\n",
    "            betas = torch.linspace(1e-4, 2e-2, self.T, dtype=torch.float32)\n",
    "        \n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "        alphas_cumprod_prev = torch.cat([torch.ones(1), alphas_cumprod[:-1]])\n",
    "\n",
    "        self.register_buffer(\"betas\", betas)\n",
    "        self.register_buffer(\"alphas\", alphas)\n",
    "        self.register_buffer(\"alphas_cumprod\", alphas_cumprod)\n",
    "        self.register_buffer(\"alphas_cumprod_prev\", alphas_cumprod_prev)\n",
    "\n",
    "        self.register_buffer(\"sqrt_alphas_cumprod\", torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer(\"sqrt_one_minus_alphas_cumprod\", torch.sqrt(1.0 - alphas_cumprod))\n",
    "\n",
    "        posterior_variance = (\n",
    "            betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
    "        )\n",
    "        self.register_buffer(\"posterior_variance\", posterior_variance) \n",
    "        self.register_buffer(\"posterior_ log_variance_clipped\", torch.log(posterior_variance.clamp(min=1e-20)))\n",
    "\n",
    "        self.register_buffer(\"sqrt_recip_alphas_cumprod\", torch.sqrt(1.0 / alphas_cumprod))\n",
    "        self.register_buffer(\"sqrt_recipm_alphas_cumprod\", torch.sqrt(1.0 / alphas_cumprod - 1.0))\n",
    "        \n",
    "        self.register_buffer(\"posterior_mean_coef1\", betas * torch.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod))\n",
    "        self.register_buffer(\"posterior_mean_coef2\", (1.0 - alphas_cumprod_prev) * torch.sqrt(betas) / (1.0 - alphas_cumprod))\n",
    "\n",
    "        self.loss_fn = Losses[loss_type]()\n",
    "\n",
    "    def q_posterior(self, x_start, x, t):\n",
    "        posterior_mean = (\n",
    "            extract(self.posterior_mean_coef1, t, x.shape) * x_start +\n",
    "            extract(self.posterior_mean_coef2, t, x.shape) * x\n",
    "        )\n",
    "        posterior_variance = extract(self.posterior_variance, t, x.shape)\n",
    "        posterior_log_variance = extract(self.posterior_log_variance_clipped, t, x.shape)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance\n",
    "\n",
    "    def predict_start_from_noise(self, x, t, pred_noise):\n",
    "        return (extract(self.sqrt_recip_alphas_cumprod, t, x.shape) * x - \n",
    "                extract(self.sqrt_recipm_alphas_cumprod, t, x.shape) * pred_noise)\n",
    "\n",
    "    def p_mean_variance(self, x, t, s):\n",
    "        pred_noise = self.model(x, t, s)\n",
    "        x_recon = self.predict_start_from_noise(x, t, pred_noise)\n",
    "        x_recon.champ_(-1, 1)\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_recon, x, t)\n",
    "        return model_mean, posterior_log_variance\n",
    "\n",
    "    def p_sample_loop(self, state, shape, *args, **kwargs):\n",
    "        '''\n",
    "        state: torch.Tensor (batch_size, state_dim)\n",
    "        '''\n",
    "        device = self.device\n",
    "        batch_size = state.size(0)\n",
    "        x = torch.randn(shape, device=device, requires_grad=False)\n",
    "\n",
    "        for i in reversed(range(0, self.T)):\n",
    "            t = torch.full((batch_size, ), i, device=device, dtype=torch.long)\n",
    "            x = self.p_sample(x, t, state)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def p_mean_variance(self, x, t, s):\n",
    "        pred_noise = self.model(x, t, s)\n",
    "\n",
    "\n",
    "    def p_sample(self, x, t, s):\n",
    "        b, *_, device = *x.shape, x.device\n",
    "        model_mean, model_log_variance = self.p_mean_variance(x, t, s)\n",
    "        noise = torch.randn_like(x)\n",
    "\n",
    "        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1, ) * (len(x.shape)-1)))\n",
    "\n",
    "        return model_mean + nonzero_mask * torch.exp(0.5 * model_log_variance) * noise\n",
    "    \n",
    "    def sample(self, state, *args, **kwargs):\n",
    "        '''\n",
    "        state: torch.Tensor (batch_size, state_dim)\n",
    "        '''\n",
    "        batch_size = state.size(0)\n",
    "        shape = [batch_size, self.action_dim]\n",
    "        action = self.p_sample_loop(state, shape, *args, **kwargs)\n",
    "        return action.clamp(-1.0, 1.0)\n",
    "\n",
    "    def q_sample(self, x_start, t, noise):\n",
    "        sample = (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
    "        )\n",
    "        return sample\n",
    "\n",
    "    def p_losses(self, x_start, state, t, weights = 1.0):\n",
    "        noise = torch.randn_like(x_start)\n",
    "        x_noisy = self.q_sample(x_start, t, noise)\n",
    "        x_recon = self.model(x_noisy, t, state)\n",
    "\n",
    "        loss = self.loss_fn(x_recon, noise, weights)\n",
    "        return loss\n",
    "    \n",
    "    def loss(self,x, state, weight = 1.0):\n",
    "        batch_size = len(x)\n",
    "        t = torch.randint(0, self.T, (batch_size, ), device=self.device).long\n",
    "        return self.p_losses(x, state, t, weight)\n",
    "    \n",
    "\n",
    "    def forward(self, state, *args, **kwargs):\n",
    "        return self.sample(state, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
